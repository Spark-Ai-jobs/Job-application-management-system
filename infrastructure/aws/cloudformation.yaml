AWSTemplateFormatVersion: '2010-09-09'
Description: 'SparkWorkforce - Single EC2 Instance Deployment'

Parameters:
  InstanceType:
    Type: String
    Default: t3.medium
    AllowedValues:
      - t3.micro
      - t3.small
      - t3.medium
      - t3.large
    Description: EC2 instance type

  KeyName:
    Type: AWS::EC2::KeyPair::KeyName
    Description: Name of an existing EC2 KeyPair for SSH access

  SSHLocation:
    Type: String
    Default: 0.0.0.0/0
    Description: IP range for SSH access (use your IP/32 for security)

  DomainName:
    Type: String
    Default: sparkworkforce.studio
    Description: Domain name for the application

Mappings:
  RegionAMI:
    us-east-1:
      AMI: ami-0c7217cdde317cfec
    us-east-2:
      AMI: ami-05fb0b8c1424f266b
    us-west-1:
      AMI: ami-0ce2cb35386fc22e9
    us-west-2:
      AMI: ami-008fe2fc65df48dac
    eu-west-1:
      AMI: ami-0905a3c97561e0b69
    eu-central-1:
      AMI: ami-0faab6bdbac9486fb
    ap-southeast-1:
      AMI: ami-078c1149d8ad719a7
    ap-northeast-1:
      AMI: ami-0d52744d6551d851e

Resources:
  # Security Group
  SparkWorkforceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for SparkWorkforce
      GroupName: sparkworkforce-sg
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: !Ref SSHLocation
          Description: SSH access
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          CidrIp: 0.0.0.0/0
          Description: HTTP
        - IpProtocol: tcp
          FromPort: 443
          ToPort: 443
          CidrIp: 0.0.0.0/0
          Description: HTTPS
      Tags:
        - Key: Name
          Value: sparkworkforce-sg

  # EC2 Instance
  SparkWorkforceInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !FindInMap [RegionAMI, !Ref 'AWS::Region', AMI]
      InstanceType: !Ref InstanceType
      KeyName: !Ref KeyName
      SecurityGroupIds:
        - !Ref SparkWorkforceSecurityGroup
      BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
            VolumeSize: 30
            VolumeType: gp3
            DeleteOnTermination: true
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -e

          # Update system
          apt-get update && apt-get upgrade -y

          # Install packages
          apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release ufw fail2ban htop git

          # Install Docker
          curl -fsSL https://get.docker.com -o get-docker.sh
          sh get-docker.sh
          systemctl enable docker
          systemctl start docker

          # Install Docker Compose
          curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
          chmod +x /usr/local/bin/docker-compose

          # Create deploy user
          useradd -m -s /bin/bash -G docker deploy
          mkdir -p /home/deploy/.ssh
          cp /home/ubuntu/.ssh/authorized_keys /home/deploy/.ssh/
          chown -R deploy:deploy /home/deploy/.ssh
          chmod 700 /home/deploy/.ssh
          chmod 600 /home/deploy/.ssh/authorized_keys

          # Setup firewall
          ufw default deny incoming
          ufw default allow outgoing
          ufw allow ssh
          ufw allow http
          ufw allow https
          ufw --force enable

          # Create application directory
          mkdir -p /opt/sparkworkforce/production
          mkdir -p /opt/sparkworkforce/backups
          chown -R deploy:deploy /opt/sparkworkforce

          # Clone repository
          su - deploy -c "git clone https://github.com/Spark-Ai-jobs/Job-application-management-system.git /opt/sparkworkforce/production"

          # Create .env file
          cat > /opt/sparkworkforce/production/.env << 'ENVEOF'
          DOMAIN=${DomainName}
          CORS_ORIGIN=https://${DomainName}
          POSTGRES_USER=spark
          POSTGRES_PASSWORD=$(openssl rand -base64 24)
          POSTGRES_DB=spark_ai
          REDIS_PASSWORD=$(openssl rand -base64 24)
          MINIO_ROOT_USER=sparkadmin
          MINIO_ROOT_PASSWORD=$(openssl rand -base64 24)
          JWT_SECRET=$(openssl rand -base64 32)
          JWT_EXPIRES_IN=7d
          NODE_ENV=production
          ATS_THRESHOLD=90
          SCRAPE_INTERVAL_HOURS=6
          SLA_TIMEOUT_MINUTES=20
          GITHUB_REPOSITORY_OWNER=spark-ai-jobs
          IMAGE_TAG=main
          ENVEOF
          chown deploy:deploy /opt/sparkworkforce/production/.env
          chmod 600 /opt/sparkworkforce/production/.env

          # Create systemd service
          cat > /etc/systemd/system/sparkworkforce.service << 'SVCEOF'
          [Unit]
          Description=SparkWorkforce Application
          Requires=docker.service
          After=docker.service

          [Service]
          Type=oneshot
          RemainAfterExit=yes
          User=deploy
          WorkingDirectory=/opt/sparkworkforce/production
          ExecStart=/usr/local/bin/docker-compose -f docker-compose.prod.yml up -d
          ExecStop=/usr/local/bin/docker-compose -f docker-compose.prod.yml down

          [Install]
          WantedBy=multi-user.target
          SVCEOF
          systemctl daemon-reload

          echo "SparkWorkforce EC2 setup complete!"

      Tags:
        - Key: Name
          Value: sparkworkforce-prod
        - Key: Application
          Value: SparkWorkforce

  # Elastic IP
  SparkWorkforceEIP:
    Type: AWS::EC2::EIP
    Properties:
      Domain: vpc
      Tags:
        - Key: Name
          Value: sparkworkforce-eip

  # Associate EIP with Instance
  SparkWorkforceEIPAssociation:
    Type: AWS::EC2::EIPAssociation
    Properties:
      InstanceId: !Ref SparkWorkforceInstance
      EIP: !Ref SparkWorkforceEIP

Outputs:
  InstanceId:
    Description: EC2 Instance ID
    Value: !Ref SparkWorkforceInstance

  PublicIP:
    Description: Elastic IP Address (use this for DNS)
    Value: !Ref SparkWorkforceEIP

  SSHCommand:
    Description: SSH command to connect
    Value: !Sub 'ssh -i ${KeyName}.pem ubuntu@${SparkWorkforceEIP}'

  ApplicationURL:
    Description: Application URL (after DNS setup)
    Value: !Sub 'https://${DomainName}'

  NextSteps:
    Description: Next steps after deployment
    Value: !Sub |
      1. Point DNS A record for ${DomainName} to ${SparkWorkforceEIP}
      2. SSH: ssh -i ${KeyName}.pem ubuntu@${SparkWorkforceEIP}
      3. Setup SSL: sudo su - deploy && cd /opt/sparkworkforce/production && ./scripts/init-ssl.sh
      4. Start app: docker-compose -f docker-compose.prod.yml up -d
