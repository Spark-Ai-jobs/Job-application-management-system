#cloud-config
# SparkWorkforce - AWS EC2 Setup
# Use this as user data when launching an EC2 instance

package_update: true
package_upgrade: true

packages:
  - apt-transport-https
  - ca-certificates
  - curl
  - gnupg
  - lsb-release
  - ufw
  - fail2ban
  - htop
  - git
  - awscli

runcmd:
  # Install Docker
  - curl -fsSL https://get.docker.com -o get-docker.sh
  - sh get-docker.sh
  - systemctl enable docker
  - systemctl start docker

  # Install Docker Compose
  - curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
  - chmod +x /usr/local/bin/docker-compose

  # Create deploy user
  - useradd -m -s /bin/bash -G docker deploy
  - mkdir -p /home/deploy/.ssh
  - cp /root/.ssh/authorized_keys /home/deploy/.ssh/ 2>/dev/null || true
  - chown -R deploy:deploy /home/deploy/.ssh
  - chmod 700 /home/deploy/.ssh
  - chmod 600 /home/deploy/.ssh/authorized_keys 2>/dev/null || true

  # Setup firewall
  - ufw default deny incoming
  - ufw default allow outgoing
  - ufw allow ssh
  - ufw allow http
  - ufw allow https
  - ufw --force enable

  # Setup fail2ban
  - systemctl enable fail2ban
  - systemctl start fail2ban

  # Create application directory
  - mkdir -p /opt/sparkworkforce/production
  - mkdir -p /opt/sparkworkforce/backups
  - chown -R deploy:deploy /opt/sparkworkforce

  # Clone repository
  - su - deploy -c "git clone https://github.com/Spark-Ai-jobs/Job-application-management-system.git /opt/sparkworkforce/production"

  # Create placeholder .env file
  - |
    cat > /opt/sparkworkforce/production/.env << 'ENVEOF'
    # SparkWorkforce Production Environment - AWS
    # IMPORTANT: Update these values before running!

    DOMAIN=sparkworkforce.studio
    CORS_ORIGIN=https://sparkworkforce.studio

    # Database - CHANGE THESE!
    POSTGRES_USER=spark
    POSTGRES_PASSWORD=CHANGE_ME_SECURE_PASSWORD
    POSTGRES_DB=spark_ai

    # Redis - CHANGE THIS!
    REDIS_PASSWORD=CHANGE_ME_SECURE_PASSWORD

    # MinIO - CHANGE THESE!
    MINIO_ROOT_USER=sparkadmin
    MINIO_ROOT_PASSWORD=CHANGE_ME_SECURE_PASSWORD

    # JWT - CHANGE THIS! (use: openssl rand -base64 32)
    JWT_SECRET=CHANGE_ME_GENERATE_SECURE_KEY
    JWT_EXPIRES_IN=7d

    # Application Settings
    NODE_ENV=production
    ATS_THRESHOLD=90
    SCRAPE_INTERVAL_HOURS=6
    SLA_TIMEOUT_MINUTES=20

    # Docker Images
    GITHUB_REPOSITORY_OWNER=spark-ai-jobs
    IMAGE_TAG=main
    ENVEOF
  - chown deploy:deploy /opt/sparkworkforce/production/.env
  - chmod 600 /opt/sparkworkforce/production/.env

write_files:
  - path: /etc/systemd/system/sparkworkforce.service
    content: |
      [Unit]
      Description=SparkWorkforce Application
      Requires=docker.service
      After=docker.service

      [Service]
      Type=oneshot
      RemainAfterExit=yes
      User=deploy
      WorkingDirectory=/opt/sparkworkforce/production
      ExecStart=/usr/local/bin/docker-compose -f docker-compose.prod.yml up -d
      ExecStop=/usr/local/bin/docker-compose -f docker-compose.prod.yml down

      [Install]
      WantedBy=multi-user.target

  - path: /etc/cron.daily/sparkworkforce-backup
    permissions: '0755'
    content: |
      #!/bin/bash
      # Daily database backup
      BACKUP_DIR=/opt/sparkworkforce/backups
      DATE=$(date +%Y%m%d_%H%M%S)

      docker exec spark_postgres pg_dump -U spark spark_ai | gzip > $BACKUP_DIR/db_backup_$DATE.sql.gz

      # Optional: Upload to S3
      # aws s3 cp $BACKUP_DIR/db_backup_$DATE.sql.gz s3://your-backup-bucket/

      # Keep only last 7 days of backups
      find $BACKUP_DIR -name "db_backup_*.sql.gz" -mtime +7 -delete

final_message: |
  SparkWorkforce AWS EC2 setup complete!

  Next steps:
  1. SSH into server: ssh -i your-key.pem ubuntu@YOUR_EC2_IP
  2. Switch to deploy user: sudo su - deploy
  3. Edit environment: nano /opt/sparkworkforce/production/.env
  4. Setup SSL: cd /opt/sparkworkforce/production && ./scripts/init-ssl.sh
  5. Start application: sudo systemctl start sparkworkforce
